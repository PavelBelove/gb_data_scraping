{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HomeWork 4\n",
    "\n",
    "## Pavel Belov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient, errors\n",
    "import zlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User agent\n",
    "\n",
    "DEFAULT_HEADER = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, \\\n",
    "like Gecko) Chrome/83.0.4103.97 Safari/537.36'\n",
    "\n",
    "# Mail.ru\n",
    "\n",
    "MAIL_DOMAIN_URL = 'https://news.mail.ru'\n",
    "MAIL_SEARCH_URL = f'{MAIL_DOMAIN_URL}/search'\n",
    "MAIL_API_URL = f'{MAIL_DOMAIN_URL}//najax/search'\n",
    "\n",
    "# Yandex\n",
    "\n",
    "YANDEX_DOMAIN_URL = 'https://newssearch.yandex.ru'\n",
    "YANDEX_SEARCH_URL = f'{YANDEX_DOMAIN_URL}/yandsearch'\n",
    "\n",
    "# Lenta\n",
    "\n",
    "LENTA_DOMAIN_URL = 'https://m.lenta.ru'\n",
    "LENTA_SEARCH_URL = f'{LENTA_DOMAIN_URL}/search/v2/process'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mail.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MailRuParser():\n",
    "    def __init__(self, params):\n",
    "        self.url = MAIL_SEARCH_URL\n",
    "        print(self.url)\n",
    "        self.params = {'q': query_string}\n",
    "        self.header = {\n",
    "            'User-Agent': DEFAULT_HEADER,\n",
    "            'Cache-Control': 'private, no-cache, no-store'\n",
    "        }\n",
    "        self.response = None\n",
    "        self.dom = None \n",
    "        \n",
    "    def fetch_data(self, url=None):\n",
    "        if self.url is None and url is None:\n",
    "            raise Exception(\"No 'url' parameter provided during initialization\")\n",
    "\n",
    "        if url is not None:\n",
    "            self.url = url\n",
    "            self.response = requests.get(url=self.url, headers=self.header)\n",
    "            return\n",
    "\n",
    "        self.response = requests.get(url=self.url, headers=self.header, params=self.params)\n",
    "\n",
    "    def get_dom(self):\n",
    "        if self.response is None:\n",
    "            raise Exception(\"You need to fetch_data first\")\n",
    "        self.dom = html.fromstring(self.response.text)\n",
    "        return self.dom\n",
    "\n",
    "    def get_news(self):\n",
    "        nodes_list = self.get_nodes_list()\n",
    "\n",
    "        news = []\n",
    "        for node in nodes_list:\n",
    "            article = {\n",
    "                'link': self.get_link_from_node(node),\n",
    "                'source': self.get_source_from_node(node),\n",
    "                'title': self.get_title_from_node(node),\n",
    "                'published_at': self.get_publish_date_from_node(node)\n",
    "            }\n",
    "            news.append(article)\n",
    "\n",
    "        return news\n",
    "\n",
    "    def get_news_from_pages_range(self, pages_range):\n",
    "        total_news = []\n",
    "        for page_number in pages_range:\n",
    "            self.fetch_data(self.get_page_link(page_number))\n",
    "            total_news += self.get_api_news()\n",
    "\n",
    "        return total_news\n",
    "\n",
    "    def get_api_news(self):\n",
    "        data = self.response.json()\n",
    "        news_list = []\n",
    "        for item in data['data']['items']:\n",
    "            article = {\n",
    "                'link': f'{MAIL_DOMAIN_URL}{item[\"url\"]}',\n",
    "                'source': item['source']['name'],\n",
    "                'title': item['title'],\n",
    "                'published_at': datetime.fromisoformat(item['published']['rfc3339'])\n",
    "            }\n",
    "            news_list.append(article)\n",
    "\n",
    "        return news_list\n",
    "\n",
    "    def get_nodes_list(self):\n",
    "        dom = self.get_dom()\n",
    "        return dom.xpath(\"//div[@class='paging js-module']//div[@class='paging__content js-pgng_cont']/\\\n",
    "        div[@class='newsitem newsitem_height_fixed js-ago-wrapper js-pgng_item']\")\n",
    "\n",
    "    def get_page_link(self, page_number):\n",
    "        return f'{MAIL_API_URL}/?q={self.params[\"q\"]}&page={page_number}'\n",
    "\n",
    "    def get_source_from_node(self, node):\n",
    "        return node.xpath(\".//span[@class='newsitem__param'][1]/text()\")[0]\n",
    "\n",
    "    def get_title_from_node(self, node):\n",
    "        return node.xpath(\"./span[@class='cell']/a/@href\")[0]\n",
    "\n",
    "    def get_link_from_node(self, node):\n",
    "        return node.xpath(\"./span[@class='cell']/a/span/text()\")[0]\n",
    "\n",
    "    def get_publish_date_from_node(self, node):\n",
    "        published_at_string = node.xpath(\".//div[@class='newsitem__params']/span[@class='newsitem__param js-ago']/@datetime\")[0]\n",
    "        return datetime.fromisoformat(published_at_string) if published_at_string is not None else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yandex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YandexParser():\n",
    "    \n",
    "    def __init__(self, query):\n",
    "        self.url = YANDEX_SEARCH_URL\n",
    "        self.params = {\n",
    "             'text': query,\n",
    "             'rpt': 'nnews2',\n",
    "             'wiz_no_news': 1,\n",
    "             'rel': 'rel'\n",
    "        }\n",
    "        self.header = {\n",
    "            'User-Agent': DEFAULT_HEADER,\n",
    "            'Cache-Control': 'private, no-cache, no-store'\n",
    "        }\n",
    "        self.response = None\n",
    "        self.dom = None\n",
    "        \n",
    "    def fetch_data(self, url=None):\n",
    "        if self.url is None and url is None:\n",
    "            raise Exception(\"No 'url' parameter provided during initialization\")\n",
    "\n",
    "        if url is not None:\n",
    "            self.url = url\n",
    "            self.response = requests.get(url=self.url, headers=self.header)\n",
    "            return\n",
    "\n",
    "        self.response = requests.get(url=self.url, headers=self.header, params=self.params)\n",
    "\n",
    "    def get_dom(self):\n",
    "        if self.response is None:\n",
    "            raise Exception(\"You need to fetch_data first\")\n",
    "        self.dom = html.fromstring(self.response.text)\n",
    "        return self.dom\n",
    "\n",
    "    def get_news(self):\n",
    "        nodes_list = self.get_nodes_list()\n",
    "\n",
    "        news = []\n",
    "        for node in nodes_list:\n",
    "            article = {\n",
    "                'link': self.get_link_from_node(node),\n",
    "                'source': self.get_source_from_node(node),\n",
    "                'title': self.get_title_from_node(node),\n",
    "                'published_at': self.get_publish_date_from_node(node)\n",
    "            }\n",
    "            news.append(article)\n",
    "\n",
    "        return news\n",
    "\n",
    "    def get_news_pages(self, number_of_pages):\n",
    "        total_news = []\n",
    "\n",
    "        self.fetch_data()\n",
    "        total_news += self.get_news()\n",
    "        next_page_link = self.get_page_link(None)\n",
    "\n",
    "        if next_page_link is None:\n",
    "            return total_news\n",
    "\n",
    "        for i in range(0, number_of_pages):\n",
    "            next_page_link = self.get_page_link(i)\n",
    "            if next_page_link is not None:\n",
    "                self.fetch_data(next_page_link)\n",
    "                total_news += self.get_news()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return total_news\n",
    "\n",
    "    def get_nodes_list(self):\n",
    "        dom = self.get_dom()\n",
    "        return dom.xpath(\"//ul[@class='search-list']/li\")\n",
    "\n",
    "    def get_page_link(self, page_number):\n",
    "        dom = self.get_dom()\n",
    "        link_node = dom.xpath(\"//div[@class='pager__content']//span[@class='pager__group'][last()]/a/@href\")\n",
    "        if link_node is not None and len(link_node) != 0:\n",
    "            link_tail = link_node[0]\n",
    "            return f'{YANDEX_DOMAIN_URL}{link_tail}'\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_source_from_node(self, node):\n",
    "        return node.xpath(\".//div[@class='document i-bem']//div[@class='document__provider-name']/text()\")[0]\n",
    "\n",
    "    def get_title_from_node(self, node):\n",
    "        return node.xpath(\".//div[@class='document i-bem']//h2[@class='document__head']//a/text()\")[0]\n",
    "\n",
    "    def get_link_from_node(self, node):\n",
    "        return node.xpath(\".//div[@class='document i-bem']//h2[@class='document__head']//a/@href\")[0]\n",
    "\n",
    "    def get_publish_date_from_node(self, node):\n",
    "        return node.xpath(\".//div[@class='document i-bem']//div[@class='document__time']/text()\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenta.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LentaParser():\n",
    "    def __init__(self, query):\n",
    "        self.url = LENTA_SEARCH_URL\n",
    "        self.params = {\n",
    "             'query': query,\n",
    "             'from': 0,\n",
    "             'size': 100,\n",
    "             'sort': 2,\n",
    "             'title_only': 0,\n",
    "             'domain': 1\n",
    "         }\n",
    "        self.header = {\n",
    "            'User-Agent': DEFAULT_HEADER,\n",
    "            'Cache-Control': 'private, no-cache, no-store'\n",
    "        }\n",
    "        self.response = None\n",
    "        self.dom = None\n",
    "        \n",
    "    def fetch_data(self, url=None):\n",
    "        if self.url is None and url is None:\n",
    "            raise Exception(\"No 'url' parameter provided during initialization\")\n",
    "\n",
    "        if url is not None:\n",
    "            self.url = url\n",
    "            self.response = requests.get(url=self.url, headers=self.header)\n",
    "            return\n",
    "\n",
    "        self.response = requests.get(url=self.url, headers=self.header, params=self.params)\n",
    "    \n",
    "    def get_dom(self):\n",
    "        if self.response is None:\n",
    "            raise Exception(\"You need to fetch_data first\")\n",
    "        self.dom = html.fromstring(self.response.text)\n",
    "        return self.dom\n",
    "\n",
    "    def get_news(self):\n",
    "        nodes_list = self.get_nodes_list()\n",
    "\n",
    "        news = []\n",
    "        for node in nodes_list:\n",
    "            article = {\n",
    "                'link': self.get_link_from_node(node),\n",
    "                'source': self.get_source_from_node(node),\n",
    "                'title': self.get_title_from_node(node),\n",
    "                'published_at': self.get_publish_date_from_node(node)\n",
    "            }\n",
    "            news.append(article)\n",
    "\n",
    "        return news\n",
    "\n",
    "    def get_news_pages(self, pages_range):\n",
    "        total_news = []\n",
    "        for page_number in pages_range:\n",
    "            self.params['from'] = page_number * self.params['size']\n",
    "            self.fetch_data()\n",
    "            total_news += self.get_news()\n",
    "\n",
    "        return total_news\n",
    "\n",
    "    def get_nodes_list(self):\n",
    "        result = self.response.json()['matches']\n",
    "        return result\n",
    "\n",
    "    def get_page_link(self, page_number):\n",
    "        pass\n",
    "\n",
    "    def get_source_from_node(self, node):\n",
    "        return None\n",
    "\n",
    "    def get_title_from_node(self, node):\n",
    "        return node['title']\n",
    "\n",
    "    def get_link_from_node(self, node):\n",
    "        return node['url']\n",
    "\n",
    "    def get_publish_date_from_node(self, node):\n",
    "        timestamp = node['pubdate']\n",
    "        return datetime.fromtimestamp(timestamp) if timestamp else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client['news_db']\n",
    "news_db = db.news\n",
    "\n",
    "\n",
    "def make_hash(any_dict):\n",
    "    return zlib.adler32(bytes(repr(any_dict), 'utf-8'))\n",
    "\n",
    "\n",
    "def save_news_to_db(news_list):\n",
    "    for article in news_list:\n",
    "        article_hash = make_hash(article)\n",
    "        article[\"_id\"] = article_hash\n",
    "\n",
    "        try:\n",
    "            news_db.insert_one(article)\n",
    "        except errors.DuplicateKeyError:\n",
    "            print(\"Duplicate found for article: \", article)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x7f7bbe419cc8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_db.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.mail.ru/search\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query_string = 'наука'\n",
    "\n",
    "# news.mail.ru/\n",
    "mail_ru_parser = MailRuParser(query_string)\n",
    "mail_ru_parser.fetch_data()\n",
    "mail_ru_news_from_page = mail_ru_parser.get_news()\n",
    "mail_ru_news_from_api = mail_ru_parser.get_news_from_pages_range(range(2, 12))\n",
    "\n",
    "# yandex.ru/news/\n",
    "\n",
    "yandex_parser = YandexParser(query_string)\n",
    "yandex_parser.fetch_data()\n",
    "yandex_news = yandex_parser.get_news_pages(3)\n",
    "\n",
    "# lenta.ru/\n",
    "\n",
    "lenta_parser = LentaParser(query_string)\n",
    "lenta_parser.fetch_data()\n",
    "lenta_news = lenta_parser.get_news_pages(range(0, 2))\n",
    "\n",
    "result_news = yandex_news + lenta_news + mail_ru_news_from_page + mail_ru_news_from_api \n",
    "\n",
    "save_news_to_db(result_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_all_news():\n",
    "    return_list = []\n",
    "    for news in news_db.find({}):\n",
    "        return_list.append(news)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 1394723968,\n",
      "  'link': 'https://www.pnp.ru/social/akademik-ran-soobshhil-chto-pandemiya-koronavirusa-mozhet-dlitsya-ot-goda-do-tryokh-let.html',\n",
      "  'published_at': '14\\xa0июня\\xa0в\\xa001:22',\n",
      "  'source': 'Парламентская газета',\n",
      "  'title': 'Академик РАН сообщил, что пандемия коронавируса может длиться от '\n",
      "           'года до трёх лет'},\n",
      " {'_id': 239513037,\n",
      "  'link': 'https://360tv.ru/news/obschestvo/koronavirus-mozhet-mutirovat-v-bolee-agressivnye-formy-akademik-ran/',\n",
      "  'published_at': '14\\xa0июня\\xa0в\\xa004:35',\n",
      "  'source': 'Телеканал 360°',\n",
      "  'title': 'Коронавирус может мутировать в более агрессивные формы — академик '\n",
      "           'РАН'},\n",
      " {'_id': 3770663438,\n",
      "  'link': 'https://tsargrad.tv/news/vakcina-na-skorost-jeto-prestupno-ukol-ot-koronavirusa-raskritikoval-doktor-medicinskih-nauk_260564',\n",
      "  'published_at': '11:00',\n",
      "  'source': 'Царьград',\n",
      "  'title': 'Вакцина на скорость - это преступно: «укол от коронавируса» '\n",
      "           'раскритиковал доктор медицинских '},\n",
      " {'_id': 2206312445,\n",
      "  'link': 'https://www.pnp.ru/social/v-minprosveshheniya-nazvali-bazovye-cennosti-shkolnogo-vospitaniya.html',\n",
      "  'published_at': '11:42',\n",
      "  'source': 'Парламентская газета',\n",
      "  'title': 'В Минпросвещения назвали базовые ценности школьного воспитания'},\n",
      " {'_id': 2429385875,\n",
      "  'link': 'https://sn.ria.ru/20200616/1572988905.html',\n",
      "  'published_at': '11:28',\n",
      "  'source': 'РИА Новости',\n",
      "  'title': 'Грибов: урока воспитания и оценок за патриотизм в школе не будет'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(get_all_news()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
