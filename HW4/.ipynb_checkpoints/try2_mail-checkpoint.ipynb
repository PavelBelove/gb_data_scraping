{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HomeWork 4\n",
    "\n",
    "## Pavel Belov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient, errors\n",
    "import zlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DEFAULT_HEADER = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, \\\n",
    "like Gecko) Chrome/83.0.4103.97 Safari/537.36'\n",
    "\n",
    "\n",
    "class NewsParser(ABC):\n",
    "    u\"\"\"Абстрактный класс парсера новостей\"\"\"\n",
    "\n",
    "    def __init__(self, url, header, params):\n",
    "        self.url = url\n",
    "        self.params = params\n",
    "        self.header = header if header else {\n",
    "            'User-Agent': DEFAULT_HEADER,\n",
    "            'Cache-Control': 'private, no-cache, no-store'\n",
    "        }\n",
    "        self.response = None\n",
    "        self.dom = None\n",
    "\n",
    "#     def fetch_data(self, url=None):\n",
    "#         if self.url is None and url is None:\n",
    "#             raise Exception(\"No 'url' parameter provided during initialization\")\n",
    "\n",
    "#         if url is not None:\n",
    "#             self.url = url\n",
    "#             self.response = requests.get(url=self.url, headers=self.header)\n",
    "#             return\n",
    "\n",
    "#         self.response = requests.get(url=self.url, headers=self.header, params=self.params)\n",
    "\n",
    "#     def get_dom(self):\n",
    "#         if self.response is None:\n",
    "#             raise Exception(\"You need to fetch_data first\")\n",
    "#         self.dom = html.fromstring(self.response.text)\n",
    "#         return self.dom\n",
    "\n",
    "#     def get_news(self):\n",
    "#         nodes_list = self.get_nodes_list()\n",
    "\n",
    "#         news = []\n",
    "#         for node in nodes_list:\n",
    "#             article = {\n",
    "#                 'link': self.get_link_from_node(node),\n",
    "#                 'source': self.get_source_from_node(node),\n",
    "#                 'title': self.get_title_from_node(node),\n",
    "#                 'published_at': self.get_publish_date_from_node(node)\n",
    "#             }\n",
    "#             news.append(article)\n",
    "\n",
    "#         return news\n",
    "\n",
    "    @staticmethod\n",
    "    def save_news_to_db(news_list):\n",
    "        save_news_to_db(news_list)\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_page_link(self, page_number):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_nodes_list(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_source_from_node(self, node):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_title_from_node(self, node):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_link_from_node(self, node):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_publish_date_from_node(self, node):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mail.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MAIL_DOMAIN_URL = 'https://news.mail.ru'\n",
    "MAIL_SEARCH_URL = f'{MAIL_DOMAIN_URL}/search'\n",
    "MAIL_API_URL = f'{MAIL_DOMAIN_URL}//najax/search'\n",
    "\n",
    "\n",
    "class MailRuParser(NewsParser, ABC):\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.url = MAIL_SEARCH_URL\n",
    "        print(self.url)\n",
    "        self.params = {'q': query_string}\n",
    "        self.header = {\n",
    "            'User-Agent': DEFAULT_HEADER,\n",
    "            'Cache-Control': 'private, no-cache, no-store'\n",
    "        }\n",
    "        self.response = None\n",
    "        self.dom = None\n",
    "    \n",
    "    \n",
    "#     def __init__(self, query_string):\n",
    "#         super().__init__(url=MAIL_SEARCH_URL,\n",
    "#                          header=None,\n",
    "#                          params={\n",
    "#                              'q': query_string\n",
    "#                          })\n",
    "        \n",
    "    def fetch_data(self, url=None):\n",
    "        if self.url is None and url is None:\n",
    "            raise Exception(\"No 'url' parameter provided during initialization\")\n",
    "\n",
    "        if url is not None:\n",
    "            self.url = url\n",
    "            self.response = requests.get(url=self.url, headers=self.header)\n",
    "            return\n",
    "\n",
    "        self.response = requests.get(url=self.url, headers=self.header, params=self.params)\n",
    "\n",
    "    def get_dom(self):\n",
    "        if self.response is None:\n",
    "            raise Exception(\"You need to fetch_data first\")\n",
    "        self.dom = html.fromstring(self.response.text)\n",
    "        return self.dom\n",
    "\n",
    "    def get_news(self):\n",
    "        nodes_list = self.get_nodes_list()\n",
    "\n",
    "        news = []\n",
    "        for node in nodes_list:\n",
    "            article = {\n",
    "                'link': self.get_link_from_node(node),\n",
    "                'source': self.get_source_from_node(node),\n",
    "                'title': self.get_title_from_node(node),\n",
    "                'published_at': self.get_publish_date_from_node(node)\n",
    "            }\n",
    "            news.append(article)\n",
    "\n",
    "        return news\n",
    "\n",
    "    def get_news_from_pages_range(self, pages_range):\n",
    "        total_news = []\n",
    "        for page_number in pages_range:\n",
    "            self.fetch_data(self.get_page_link(page_number))\n",
    "            total_news += self.get_api_news()\n",
    "\n",
    "        return total_news\n",
    "\n",
    "    def get_api_news(self):\n",
    "        data = self.response.json()\n",
    "        news_list = []\n",
    "        for item in data['data']['items']:\n",
    "            article = {\n",
    "                'link': f'{MAIL_DOMAIN_URL}{item[\"url\"]}',\n",
    "                'source': item['source']['name'],\n",
    "                'title': item['title'],\n",
    "                'published_at': datetime.fromisoformat(item['published']['rfc3339'])\n",
    "            }\n",
    "            news_list.append(article)\n",
    "\n",
    "        return news_list\n",
    "\n",
    "    # Extension methods\n",
    "\n",
    "    def get_nodes_list(self):\n",
    "        dom = self.get_dom()\n",
    "        return dom.xpath(\"//div[@class='paging js-module']//div[@class='paging__content js-pgng_cont']/\\\n",
    "        div[@class='newsitem newsitem_height_fixed js-ago-wrapper js-pgng_item']\")\n",
    "\n",
    "    def get_page_link(self, page_number):\n",
    "        return f'{MAIL_API_URL}/?q={self.params[\"q\"]}&page={page_number}'\n",
    "\n",
    "    def get_source_from_node(self, node):\n",
    "        return node.xpath(\".//span[@class='newsitem__param'][1]/text()\")[0]\n",
    "\n",
    "    def get_title_from_node(self, node):\n",
    "        return node.xpath(\"./span[@class='cell']/a/@href\")[0]\n",
    "\n",
    "    def get_link_from_node(self, node):\n",
    "        return node.xpath(\"./span[@class='cell']/a/span/text()\")[0]\n",
    "\n",
    "    def get_publish_date_from_node(self, node):\n",
    "        published_at_string = node.xpath(\".//div[@class='newsitem__params']/span[@class='newsitem__param js-ago']/@datetime\")[0]\n",
    "        return datetime.fromisoformat(published_at_string) if published_at_string is not None else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lenta.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_URL = 'https://m.lenta.ru'\n",
    "SEARCH_URL = f'{DOMAIN_URL}/search/v2/process'\n",
    "\n",
    "# Не получается сделать парсинг новостей Lenta.ru через xpath\n",
    "# на клиенте - react, который загружает только пустой контейнер для отображения новостей\n",
    "# сами новости загружаются отдельным GET запросом к api и возвращаются в формате JSON\n",
    "\n",
    "\n",
    "class LentaParser(NewsParser, ABC):\n",
    "    def __init__(self, query_string):\n",
    "        super().__init__(url=SEARCH_URL,\n",
    "                         header=None,\n",
    "                         params={\n",
    "                             'query': query_string,\n",
    "                             'from': 0,\n",
    "                             'size': 100,\n",
    "                             'sort': 2,\n",
    "                             'title_only': 0,\n",
    "                             'domain': 1\n",
    "                         })\n",
    "\n",
    "    def get_news_pages(self, pages_range):\n",
    "        total_news = []\n",
    "        for page_number in pages_range:\n",
    "            self.params['from'] = page_number * self.params['size']\n",
    "            self.fetch_data()\n",
    "            total_news += self.get_news()\n",
    "\n",
    "        return total_news\n",
    "\n",
    "    # Extension methods\n",
    "\n",
    "    def get_nodes_list(self):\n",
    "        result = self.response.json()['matches']\n",
    "        return result\n",
    "\n",
    "    def get_page_link(self, page_number):\n",
    "        pass\n",
    "\n",
    "    def get_source_from_node(self, node):\n",
    "        return None\n",
    "\n",
    "    def get_title_from_node(self, node):\n",
    "        return node['title']\n",
    "\n",
    "    def get_link_from_node(self, node):\n",
    "        return node['url']\n",
    "\n",
    "    def get_publish_date_from_node(self, node):\n",
    "        timestamp = node['pubdate']\n",
    "        return datetime.fromtimestamp(timestamp) if timestamp else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yandex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DOMAIN_URL = 'https://newssearch.yandex.ru'\n",
    "SEARCH_URL = f'{DOMAIN_URL}/yandsearch'\n",
    "\n",
    "# Яндекс беспощадно банит ( очень неудобно собирать с него новости\n",
    "\n",
    "\n",
    "class YandexParser(NewsParser, ABC):\n",
    "    def __init__(self, query_string):\n",
    "        super().__init__(url=SEARCH_URL,\n",
    "                         header=None,\n",
    "                         params={\n",
    "                             'text': query_string,\n",
    "                             'rpt': 'nnews2',\n",
    "                             'wiz_no_news': 1,\n",
    "                             'rel': 'rel'\n",
    "                         })\n",
    "\n",
    "    def get_news_pages(self, number_of_pages):\n",
    "        total_news = []\n",
    "\n",
    "        self.fetch_data()\n",
    "        total_news += self.get_news()\n",
    "        next_page_link = self.get_page_link(None)\n",
    "\n",
    "        if next_page_link is None:\n",
    "            return total_news\n",
    "\n",
    "        for i in range(0, number_of_pages):\n",
    "            next_page_link = self.get_page_link(i)\n",
    "            if next_page_link is not None:\n",
    "                self.fetch_data(next_page_link)\n",
    "                total_news += self.get_news()\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return total_news\n",
    "\n",
    "    # Extension methods\n",
    "\n",
    "    def get_nodes_list(self):\n",
    "        dom = self.get_dom()\n",
    "        return dom.xpath(\"//ul[@class='search-list']/li\")\n",
    "\n",
    "    def get_page_link(self, page_number):\n",
    "        dom = self.get_dom()\n",
    "        link_node = dom.xpath(\"//div[@class='pager__content']//span[@class='pager__group'][last()]/a/@href\")\n",
    "        if link_node is not None and len(link_node) != 0:\n",
    "            link_tail = link_node[0]\n",
    "            return f'{DOMAIN_URL}{link_tail}'\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_source_from_node(self, node):\n",
    "        return node.xpath(\".//div[@class='document i-bem']//div[@class='document__provider-name']/text()\")[0]\n",
    "\n",
    "    def get_title_from_node(self, node):\n",
    "        return node.xpath(\".//div[@class='document i-bem']//h2[@class='document__head']//a/text()\")[0]\n",
    "\n",
    "    def get_link_from_node(self, node):\n",
    "        return node.xpath(\".//div[@class='document i-bem']//h2[@class='document__head']//a/@href\")[0]\n",
    "\n",
    "    def get_publish_date_from_node(self, node):\n",
    "        return node.xpath(\".//div[@class='document i-bem']//div[@class='document__time']/text()\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)\n",
    "db = client['news_db']\n",
    "news_db = db.news\n",
    "\n",
    "\n",
    "def make_hash(any_dict):\n",
    "    return zlib.adler32(bytes(repr(any_dict), 'utf-8'))\n",
    "\n",
    "\n",
    "def save_news_to_db(news_list):\n",
    "    for article in news_list:\n",
    "        article_hash = make_hash(article)\n",
    "        article[\"_id\"] = article_hash\n",
    "\n",
    "        try:\n",
    "            news_db.insert_one(article)\n",
    "        except errors.DuplicateKeyError:\n",
    "            print(\"Duplicate found for article: \", article)\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult object at 0x7f865fd201c8>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_db.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.mail.ru/search\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query_string = 'наука'\n",
    "\n",
    "# news.mail.ru/\n",
    "mail_ru_parser = MailRuParser(query_string)\n",
    "mail_ru_parser.fetch_data()\n",
    "mail_ru_news_from_page = mail_ru_parser.get_news()\n",
    "mail_ru_news_from_api = mail_ru_parser.get_news_from_pages_range(range(2, 12))\n",
    "\n",
    "# yandex.ru/news/\n",
    "\n",
    "# yandex_parser = YandexParser(query_string)\n",
    "# yandex_parser.fetch_data()\n",
    "# yandex_news = yandex_parser.get_news_pages(3)\n",
    "\n",
    "# lenta.ru/\n",
    "\n",
    "# lenta_parser = LentaParser(query_string)\n",
    "# lenta_parser.fetch_data()\n",
    "# lenta_news = lenta_parser.get_news_pages(range(0, 2))\n",
    "\n",
    "result_news = mail_ru_news_from_page + mail_ru_news_from_api # + yandex_news # + lenta_news\n",
    "\n",
    "save_news_to_db(result_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_all_news():\n",
    "    return_list = []\n",
    "    for news in news_db.find({}):\n",
    "        return_list.append(news)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 2679729973,\n",
      "  'link': 'Президента Арктической академии ',\n",
      "  'published_at': datetime.datetime(2020, 6, 15, 6, 50, 15),\n",
      "  'source': 'Интерфакс',\n",
      "  'title': '/incident/42185705/'},\n",
      " {'_id': 3458175019,\n",
      "  'link': 'Врач назвала распространенное осложнение у\\xa0переболевших COVID-19',\n",
      "  'published_at': datetime.datetime(2020, 6, 15, 4, 39, 26),\n",
      "  'source': 'Известия',\n",
      "  'title': '/society/42184220/'},\n",
      " {'_id': 3577000354,\n",
      "  'link': '\\ufeffКатастрофа «Норникеля»\\xa0— только начало. Полстраны под '\n",
      "          'угрозой из-за\\xa0таяния вечной мерзлоты',\n",
      "  'published_at': datetime.datetime(2020, 6, 14, 23, 39, 14),\n",
      "  'source': 'Life.ru',\n",
      "  'title': '/society/42182462/'},\n",
      " {'_id': 698398278,\n",
      "  'link': 'Нобелевские лауреаты попросили Совет Европы поддержать лидера '\n",
      "          'карельского «Мемориала»',\n",
      "  'published_at': datetime.datetime(2020, 6, 14, 20, 6, 31),\n",
      "  'source': 'Интерфакс',\n",
      "  'title': '/society/42182910/'},\n",
      " {'_id': 2460782732,\n",
      "  'link': 'Россиянам дали советы по\\xa0подготовке ко\\xa0второй волне '\n",
      "          'коронавируса',\n",
      "  'published_at': datetime.datetime(2020, 6, 14, 6, 40, 28),\n",
      "  'source': 'Lenta.Ru',\n",
      "  'title': '/society/42177796/'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(get_all_news()[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
